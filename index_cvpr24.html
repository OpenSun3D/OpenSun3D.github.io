<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Mask3D">
  <meta name="keywords" content="OpenSun3D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Open‚òÄÔ∏è3D </title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="css/style.css"> <!-- Resource style -->
  <script src="js/modernizr.js"></script> <!-- Modernizr -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>

  <style>
    .rcorners1 {
      border-radius: 10px;
      background: #ffffffd0;
      padding: 5px;
      font-size: 120%;
      color: #5c5c5c;
    }

    .button {
      border-radius: 10px;
      background: #ffffffd0;
      padding: 5px 15px 5px 15px;
    }

    .dropdown {
      position: relative;
      display: inline-block;
    }

    .dropdown-content {
      display: none;
      position: absolute;
      /* background-color: #f9f9f9; */
      min-width: 140px;
      box-shadow: 0px 8px 16px 0px rgba(0, 0, 0, 0.2);
      padding: 5px 15px 5px 15px;
      z-index: 1;
    }

    .dropdown:hover .dropdown-content {
      display: block;
    }
  </style>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">‚òÄÔ∏è OpenSUN 3D üåç<br />
              <p class="title is-3 publication-title">2<sup>nd</sup> Workshop on Open-Vocabulary 3D Scene Understanding
              </p>
            </h1>
            <h1 class="is-is-5" style="color: #5c5c5c;">in conjunction with CVPR 2024, Seattle, USA.</h1>
            <b>June 18 Tuesday (1:30 PM ‚Äì 5:30 PM) in Arch 211</b>
            <br>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          <a href="#challenge" class="button">Challenge</a>
          <a href="#speakers" class="button">Speakers</a>
          <a href="#schedule" class="button">Schedule</a>
          <a href="#relatedwork" class="button">Related Work</a>
          <a href="#dates" class="button">Dates</a>
          <!-- <a href="#accepted" class="button">Accepted Papers</a> -->
          <a href="#organizers" class="button">Organizers</a>
          <div class="dropdown">
            <span class="button"><i class="fa fa-bars"></i></span>
            <div class="dropdown-content">
              <a href="index.html">ECCV 2024</a>
              <a href="index_cvpr24.html">CVPR 2024</a>
              <a href="index_iccv23.html">ICCV 2023</a>
            </div>
          </div>
        </h2>

      </div>
    </div>
  </section>

  <section class="section" style="margin-top: -50px">
    <div class="container is-max-desktop">
      <section class="section" id="Motivation">
        <div class="container is-max-desktop content">
          <h2 class="title">Motivation üí°</h2>
          <div class="content has-text-justified">
            The ability to perceive, understand and interact with arbitrary 3D environments is a long-standing goal in
            both academia and industry with applications in AR/VR as well as robotics.
            Current 3D scene understanding models are largely limited to recognizing a closed set of pre-defined object
            classes.
            Recently, large visual-language models, such as CLIP, have demonstrated impressive capabilities trained
            solely on internet-scale image-language pairs.
            Some initial works have shown that these models have the potential to extend 3D scene understanding not only
            to open set recognition, but also offer additional applications such as affordances, materials, activities,
            and properties of unseen environments.
            The goal of this workshop is to bundle these siloed efforts and to discuss and establish clear task
            definitions, evaluation metrics, and benchmark datasets.
          </div>
        </div>
      </section>
    </div>
  </section>

  <section class="section" style="margin-top: -50px">
    <div class="container is-max-desktop">
      <section class="section" id="schedule">
        <div class="container is-max-desktop content">
          <h2 class="title">Schedule ‚è∞ (tentative)</h2>
          <div class="content has-text-justified">
            <table class="table table-striped">
              <tr>
                <td width="130">13:30 - 13:45</td>
                <td width="300" style="background-color:#e4ffc2">Welcome & Introduction</td>
                <td></td>
              </tr>
              <tr>
                <td>13:45 - 14:15</td>
                <td style="background-color:#cae1ff">Keynote 1 <b></b></td>
                <td><a href="https://www.cs.utexas.edu/users/grauman/">Kristen Grauman</a> (Uni. of Texas at Austin)</td>
              </tr>
              <tr>
                <td>14:15 - 14:45</td>
                <td style="background-color:#cae1ff">Keynote 2 <b></b></td>
                <td><a href="https://chungmin99.github.io">Chung Min Kim</a>, <a href="https://kerrj.github.io">Justin Kerr</a> (UC Berkeley)</td>
              </tr>
              <tr>
                <td>14:45 - 15:00</td>
                <td style="background-color:#cae1ff">Winner Presentations<b></b></td>
                <td><b>Track 1:</b> VinAI-3DIS <b>Track 2:</b> PICO-MR (2024) </td>
              </tr>
              <tr>
                <td>15:00 - 15:45</td>
                <td style="background-color:#ffe0c6">Poster Session & Coffee Break</td>
                <td></td>
              </tr>
              <tr>
                <td>15:45 - 16:15</td>
                <td style="background-color:#cae1ff">Keynote 3 <b></b></td>
                <td><a href="https://jiajunwu.com">Jiajun Wu</a> (Stanford University)</td>
              </tr>
              <tr>
                <td>16:15 - 16:45</td>
                <td style="background-color:#cae1ff">Keynote 4<b></b></td>
                <td><a href="https://investors.matterport.com/management/dave-gausebeck">Dave Gausebeck</a> (Matterport)</td>
              </tr>
              <tr>
                <td>16:45 - 17:00</td>
                <td style="background-color:#e4ffc2">Closing</td>
                <td></td>
              </tr>
            </table>

          </div>
        </div>
      </section>
    </div>
  </section>

  <section class="section" style="margin-top: -50px">
    <div class="container is-max-desktop">
      <section class="section" id="Challenge">
        <div class="container is-max-desktop content">
          <h2 class="title" id="challenge">Challenge üöÄ</h2>
          This year, our challenge will consist of two tracks, open-vocabulary 3D object instance search and
          open-vocabulary 3D functionality grounding.
          <br>

          <ul>
            <li><b>Challenge Track 1</b>: <a
                href="https://opensun3d.github.io/cvpr24-challenge/track_1">Open-vocabulary 3D Object Instance
                Search</a></li>
            <ul>
              <li><b>Submission Portal</b>: <a
                  href="https://eval.ai/web/challenges/challenge-page/2102/overview">EvalAI</a></li>
              <li><b>Data Instructions & Helper Scripts</b>: April 17, 2024</li>
              <li><b>Dev Phase Start</b>: April 17, 2024</li>
              <li><b>Submission Portal Start</b>: April 19, 2024</li>
              <li><b>Test Phase Start</b>: May 1, 2024</li>
              <li><b>Test Phase End</b>: June 14, 2024 (14:00 Pacific Time)</li>
            </ul>
            <li><b>Challenge Track 2</b>: <a
                href="https://opensun3d.github.io/cvpr24-challenge/track_2">Open-vocabulary 3D Functionality
                Grounding</a></li>
            <ul>
              <li><b>Submission Portal</b>: <a
                  href="https://eval.ai/web/challenges/challenge-page/2262/overview">EvalAI</a></li>
              <li><b>Data Instructions & Helper Scripts</b>: April 17, 2024</li>
              <li><b>Dev Phase Start</b>: April 17, 2024</li>
              <li><b>Submission Portal Start</b>: April 19, 2024</li>
              <li><b>Test Phase Start</b>: May 4, 2024</li>
              <li><b>Test Phase End</b>: June 14, 2024 (14:00 Pacific Time)</li>
            </ul>
          </ul>

          <p>Please check <a href="https://opensun3d.github.io/index_iccv23.html#challengeresults">this page</a>
            out
            for an overview of last year's challenge results.
            We have also published a <a href="https://arxiv.org/abs/2402.15321"> technical report</a> providing an
            overview of our ICCV 2023 workshop challenge.</p>

          <p>Our workshop challenge is proudly supported by:</p>
          <p align="center">
            <img src="static/images/matterport.png" width="40%" style="margin: 20px" />
            <img src="static/images/nvidia.png" width="40%" style="margin: 20px" />
          </p>
        </div>
      </section>
    </div>
  </section>

  <section class="section" id="Invited Speakers">
    <div class="container is-max-desktop content">
      <h2 class="title" id="speakers">Invited Speakers üßë‚Äçüè´</h2>

      <a href="https://www.cs.utexas.edu/users/grauman/" target="_blank">
        <div class="card">
          <div class="card-content">
            <div class="columns is-vcentered">
              <div class="column is-one-quarter">
                <figure class="image is-128x128">
                  <img class="is-rounded" src="static/img/kristen.jpg">
                </figure>
              </div>
              <div class="column">
                <p class="title is-4">Kristen Grauman</p>
                <p class="subtitle is-6">University of Texas at Austin</p>
              </div>
            </div>
            <div class="content">
              Kristen Grauman is a Professor in the Department of Computer Science at the University of Texas at
              Austin and a Research Director in Facebook AI Research (FAIR).
              Her research in computer vision and machine learning focuses on video, visual recognition, and
              action for perception or embodied AI.
              Before joining UT-Austin in 2007, she received her Ph.D. at MIT.
              She is an IEEE Fellow, AAAI Fellow, Sloan Fellow, a Microsoft Research New Faculty Fellow, and a
              recipient of NSF CAREER and ONR Young Investigator awards,
              the PAMI Young Researcher Award in 2013, the 2013 Computers and Thought Award from the
              International Joint Conference on Artificial Intelligence (IJCAI),
              the Presidential Early Career Award for Scientists and Engineers (PECASE) in 2013.
              She was inducted into the UT Academy of Distinguished Teachers in 2017.
              She and her collaborators have been recognized with several Best Paper awards in computer vision,
              including a 2011 Marr Prize and a 2017 Helmholtz Prize (test of time award).
            </div>
          </div>
        </div>
      </a>

      <a href="https://jiajunwu.com/" target="_blank">
        <div class="card">
          <div class="card-content">
            <div class="columns is-vcentered">
              <div class="column is-one-quarter">
                <figure class="image is-128x128">
                  <img class="is-rounded" src="static/img/jiajun.jpg">
                </figure>
              </div>
              <div class="column">
                <p class="title is-4">Jiajun Wu</p>
                <p class="subtitle is-6">Stanford University</p>
              </div>
            </div>
            <div class="content">
              Jiajun Wu is an Assistant Professor of Computer Science at Stanford University, working on
              computer vision, machine learning, and computational cognitive science.
              Before joining Stanford, he was a Visiting Faculty Researcher at Google Research.
              He received his PhD in Electrical Engineering and Computer Science from the Massachusetts
              Institute of Technology.
              Wu's research has been recognized through the Young Investigator Programs (YIP) by ONR and by
              AFOSR, paper awards and finalists at ICCV, CVPR, SIGGRAPH Asia, CoRL, and IROS, dissertation
              awards from ACM, AAAI, and MIT,
              the 2020 Samsung AI Researcher of the Year, and faculty research awards from J.P. Morgan, Samsung,
              Amazon, and Meta.
            </div>
          </div>
        </div>
      </a>

      <a href="https://chungmin99.github.io" target="_blank">
        <div class="card">
          <div class="card-content">
            <div class="columns is-vcentered">
              <div class="column is-one-quarter">
                <figure class="image is-128x128">
                  <img class="is-rounded" src="static/img/cmk.jpg">
                </figure>
              </div>
              <div class="column">
                <p class="title is-4">Chung Min Kim</p>
                <p class="subtitle is-6">University of California, Berkeley</p>
              </div>
            </div>
            <div class="content">
              Chung Min Kim is a PhD student at UC Berkeley,
              where she is advised by Ken Goldberg and Angjoo Kanazawa.
              She received her dual B.S. degree in EECS (Electrical Engineering and Computer Science) and
              Mechanical Engineering from UC Berkeley in 2021.
              She is currently funded by the NSF GRFP.
              Her research interests include 3D scene understanding for computer vision and robotics.
              In particular, she is interested in modeling multi-scale semantics with 3D, using large
              vision-language models.
              Her goal is to apply these models to robots in the real world, which is challenging due to lack of
              structure and large variability in the real world.
            </div>
          </div>
        </div>
      </a>

      <a href="https://kerrj.github.io" target="_blank">
        <div class="card">
          <div class="card-content">
            <div class="columns is-vcentered">
              <div class="column is-one-quarter">
                <figure class="image is-128x128">
                  <img class="is-rounded" src="static/img/justin.jpg">
                </figure>
              </div>
              <div class="column">
                <p class="title is-4">Justin Kerr</p>
                <p class="subtitle is-6">University of California, Berkeley</p>
              </div>
            </div>
            <div class="content">
              Justin Kerr is a PhD student at UC Berkeley co-advised by Ken Goldberg and Angjoo Kanazawa working
              primarily on NeRF for robot manipulation, 3D scene understanding, and visuo-tactile representation
              learning.
              Recently Justin is interested in leveraging NeRF for language grounding, and how it could change
              how we interact with 3D.
              His work is supported by the NSF GRFP. Previously he finished my bachelor's at CMU where he worked
              with Howie Choset on multi-agent path planning, and spent time at Berkshire Grey and NASA's JPL.
            </div>
          </div>
        </div>
      </a>

      <a href="https://investors.matterport.com/management/dave-gausebeck" target="_blank">
        <div class="card">
          <div class="card-content">
            <div class="columns is-vcentered">
              <div class="column is-one-quarter">
                <figure class="image is-128x128">
                  <img class="is-rounded" src="static/img/dave.jpg">
                </figure>
              </div>
              <div class="column">
                <p class="title is-4">Dave Gausebeck</p>
                <p class="subtitle is-6">Matterport</p>
              </div>
            </div>
            <div class="content">
              Dave Gausebeck is the co-founder and Chief Scientist Officer of Matterport, leading the technological research and operations at Matterport.

              As one of Matterport's founders, Dave developed much of the computer vision technology that makes Matterport tick. He continues to develop and improve those algorithms alongside a great team of vision researchers and software engineers.
              
              Dave was at PayPal when the whole company fit into a single conference room, building core back-end and security systems as well as developing the first commercial implementation of a CAPTCHA.
              
              Dave earned a BS in Computer Science from the University of Illinois at Urbana-Champaign.
            </div>
          </div>
        </div>
      </a>

    </div>
  </section>

  <section class="section" id="Invited Speakers">
    <div class="container is-max-desktop content">
      <h2 class="title" id="relatedwork">Related Works üßë‚Äçü§ù</h2>
      Below is a collection of concurrent and related works in the field of open-set 3D scene understanding.
      Please
      feel free to get in touch to add other works as well.
      <ul>
        <li><a href="https://concept-fusion.github.io/">ConceptFusion: Open-set Multimodal 3D Mapping</a> RSS'23
        </li>
        <li><a href="https://pengsongyou.github.io/openscene">OpenScene: 3D Scene Understanding with Open
            Vocabularies</a> CVPR'23</li>
        <li><a href="https://www.lerf.io">LERF: Language Embedded Radiance Fields</a> ICCV'23</li>
        <li><a href="https://pfnet-research.github.io/distilled-feature-fields/">Decomposing NeRF for Editing
            via Feature Field Distillation</a> NeurIPS'22</li>
        <li><a href="https://semantic-abstraction.cs.columbia.edu/">Semantic Abstraction: Open-World 3D Scene
            Understanding from 2D Vision-Language Models</a> CoRL'22</li>
        <li><a href="https://rozdavid.github.io/scannet200">Language-Grounded Indoor 3D Semantic Segmentation in
            the Wild</a> ECCV'22</li>
        <li><a href="https://3dlg-hcvc.github.io/multiscan/">MultiScan: Scalable RGBD Scanning for 3D
            Environments with Articulated Objects</a> NeurIPS'22</li>
        <li><a href="https://github.com/NVlabs/ODISE">ODISE: Open-Vocabulary Panoptic Segmentation with
            Text-to-Image Diffusion Models</a> CVPR'23</li>
        <li><a href="https://github.com/Kunhao-Liu/3D-OVS">Weakly Supervised 3D Open-Vocabulary Segmentation</a>
          NeurIPS'23</li>
        <li><a href="https://arxiv.org/abs/2306.02329">Multi-CLIP: Contrastive Vision-Language Pre-training for
            Question Answering tasks in 3D Scenes</a></li>
        <li><a href="https://openmask3d.github.io">OpenMask3D: Open-Vocabulary 3D Instance Segmentation</a>
          NeurIPS'23</li>
        <li><a href="https://arxiv.org/abs/2303.04748">CLIP-FO3D: Learning Free Open-world 3D Scene
            Representations from 2D Dense CLIP</li>
        <li><a href="https://tsagkas.github.io/vl-fields/">VL-Fields: Towards Language-Grounded Neural Implicit
            Spatial Representations</a> ICRA'23</li>
        <li><a
            href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_PLA_Language-Driven_Open-Vocabulary_3D_Scene_Understanding_CVPR_2023_paper.pdf">PLA:
            Language-Driven Open-Vocabulary 3D Scene Understanding</a> CVPR'23</li>
        <li><a href="https://arxiv.org/abs/2304.00962">RegionPLC: Regional Point-Language Contrastive Learning
            for Open-World 3D Scene Understanding</a></li>
        <li><a href="https://arxiv.org/abs/2309.00616">OpenIns3D: Snap and Lookup for 3D open-vocabulary
            Instance Segmentation</a></li>
        <li><a href="https://concept-graphs.github.io/assets/pdf/2023-ConceptGraphs.pdf">ConceptGraphs:
            Open-Vocabulary 3D Scene Graphs for Perception and Planning</a></li>
        <li><a
            href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf">Open-Vocabulary
            Point-Cloud Object Detection without 3D Annotation</a> CVPR'23</li>
        <li><a href="https://github.com/yangcaoai/CoDA_NeurIPS2023">CoDA: Collaborative Novel
            Box Discovery and Cross-modal Alignment for Open-vocabulary 3D Object Detection</a> NeurIPS'23
        <li><a href="https://opennerf.github.io">OpenNeRF: OpenSet 3D Neural Scene Segmentation with Pixel-wise
            Features and Rendered Novel Views</a> ICLR'24
        <li><a href="https://scenefun3d.github.io">SceneFun3D: Fine-Grained Functionality and Affordance
            Understanding in 3D Scenes</a> CVPR'24


        </li>
        <br>
      </ul>
      and many more ...


      </ul>
    </div>
  </section>
  
  <section class="section" id="Papers">
    <div class="container is-max-desktop content">
      <h2 class="title" id="dates">Important Dates üóìÔ∏è</h2>

      <b>Paper Track</b>: We accept novel full 8-page papers for publication in the proceedings, and
      either shorter
      4-page extended abstracts or 8-page papers of novel or previously published work that will not
      be included in
      the
      proceedings. All submissions have to follow the <a
        href="https://cvpr.thecvf.com/Conferences/2024/AuthorGuidelines">CVPR 2024 author
        guidelines.</a>
      </li>
      <ul>
        <li><b>Submission Portal</b>: <a href="https://cmt3.research.microsoft.com/OpenSUN3D2024">CMT</a></li>
        <li><b>Paper Submission Deadline</b>: April 1, 2024 (23:59 Pacific Time)</li>
        <li><b>Notification to Authors</b>: April 9, 2024</li>
        <li><b>Camera-ready submission</b>: April 14, 2024</li>
      </ul>
    </div>
  </section>

  <section class="section" id="Papers">
    <div class="container is-max-desktop content">
      <h2 class="title" id="accepted">Accepted Papers üìÑ</h2>

      <div class="box">
        <article class="media">
          <div class="media-content">
            <div class="content">
              <p>
                <strong>AffordanceLLM: Grounding Affordance from Vision Language Models</strong>
                <br>
                Shengyi Qian, Weifeng Chen, Min Bai, Xiong Zhou, Zhuowen Tu, Li Erran Li
              </p>
            </div>
            <nav class="level is-mobile">
              <div class="level-left">
                <a class="level-item" aria-label="reply" href="https://arxiv.org/pdf/2401.06341.pdf">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  Paper
                </a>
                <a class="level-item" aria-label="retweet"
                  href="https://fouheylab.eecs.umich.edu/~syqian/affordancellm/affordancellm_poster.pdf">
                  <span class="icon">
                    <i class="fas fa-chalkboard-teacher"></i>
                  </span>
                  Poster
                </a>
              </div>
            </nav>
          </div>
        </article>
      </div>

      <div class="box">
        <article class="media">
          <div class="media-content">
            <div class="content">
              <p>
                <strong>Zero-shot Dual-Path Integration Framework for Open-Vocabulary 3D Instance
                  Segmentation</strong>
                <br>
                Tri Ton, Ji Woo Hong, SooHwan Eom, Jun Yeop Shim, Junyeong Kim, Chang D. Yoo
              </p>
            </div>
            <nav class="level is-mobile">
              <div class="level-left">
                <a class="level-item" aria-label="reply"
                  href="https://drive.google.com/file/d/1Axu5tY2kxfifI2Gc7MOPsooT76cUF-Pj/view">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  Paper
                </a>
                <a class="level-item" aria-label="retweet"
                  href="https://drive.google.com/file/d/1IHfAbKWYBb_A-yRRW80j43sHA6ZMerLQ/view">
                  <span class="icon">
                    <i class="fas fa-chalkboard-teacher"></i>
                  </span>
                  Poster
                </a>
              </div>
            </nav>
          </div>
        </article>
      </div>

      <div class="box">
        <article class="media">
          <div class="media-content">
            <div class="content">
              <p>
                <strong>Auto-Vocabulary Segmentation for LiDAR Points</strong>
                <br>
                Weijie Wei, Osman √úlger, Fatemeh Karimi Nejadasl, Theo Gevers, Martin R. Oswald
              </p>
            </div>
            <nav class="level is-mobile">
              <div class="level-left">
                <a class="level-item" aria-label="reply"
                  href="https://drive.google.com/file/d/1hDA4zMHWgcZpLJ-UpxgwYaYyWKv_x92p/view">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  Paper
                </a>
                <a class="level-item" aria-label="retweet"
                  href="https://drive.google.com/file/d/146qArUTyQJyDX9aIAF9V_hBo-hvaNHL8/view">
                  <span class="icon">
                    <i class="fas fa-chalkboard-teacher"></i>
                  </span>
                  Poster
                </a>
              </div>
            </nav>
          </div>
        </article>
      </div>

      <div class="box">
        <article class="media">
          <div class="media-content">
            <div class="content">
              <p>
                <strong>Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and
                  Open-Set Relationships</strong>
                <br>
                Sebastian Koch, Narunas Vaskevicius, Mirco Colosi, Pedro Hermosilla Casajus, Timo Ropinski
              </p>
            </div>
            <nav class="level is-mobile">
              <div class="level-left">
                <a class="level-item" aria-label="reply"
                  href="https://kochsebastian.com/media/open3dsg/open3dsg_camera.pdf">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  Paper
                </a>
                <a class="level-item" aria-label="retweet"
                  href="https://kochsebastian.com/media/open3dsg/cvpr_open3dsg_12206_poster.pdf">
                  <span class="icon">
                    <i class="fas fa-chalkboard-teacher"></i>
                  </span>
                  Poster
                </a>
              </div>
            </nav>
          </div>
        </article>
      </div>

      <div class="box">
        <article class="media">
          <div class="media-content">
            <div class="content">
              <p>
                <strong>ODIN: A Single Model for 2D and 3D Segmentation</strong>
                <br>
                Ayush Jain, Pushkal Katara, Nikolaos Gkanatsios, Adam Harley, Gabriel Sarch, Kriti Aggarwal,
                Vishrav Chaudhary, Katerina
                Fragkiadaki
              </p>
            </div>
            <nav class="level is-mobile">
              <div class="level-left">
                <a class="level-item" aria-label="reply" href="https://arxiv.org/abs/2401.02416">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  Paper
                </a>
              </div>
            </nav>
          </div>
        </article>
      </div>

      <div class="box">
        <article class="media">
          <div class="media-content">
            <div class="content">
              <p>
                <strong>QueSTMaps: Queryable Semantic Topological Maps for 3D Scene Understanding</strong>
                <br>
                Kumaraditya Gupta, Rohit Jayanti, Yash Mehan, Anirudh Govil, Sourav Garg, Madhava Krishna
              </p>
            </div>
            <nav class="level is-mobile">
              <div class="level-left">
                <a class="level-item" aria-label="reply"
                  href="https://quest-maps.github.io/static/camera_ready/questmaps-cvprw.pdf">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  Paper
                </a>
                <a class="level-item" aria-label="retweet"
                  href="https://quest-maps.github.io/static/poster/questmaps-poster-v3.pdf">
                  <span class="icon">
                    <i class="fas fa-chalkboard-teacher"></i>
                  </span>
                  Poster
                </a>
              </div>
            </nav>
          </div>
        </article>
      </div>

      <div class="box">
        <article class="media">
          <div class="media-content">
            <div class="content">
              <p>
                <strong>Situational Awareness Matters in 3D Vision Language Reasoning</strong>
                <br>
                Yunze Man, Liang-Yan Gui, Yu-Xiong Wang
              </p>
            </div>
            <nav class="level is-mobile">
              <div class="level-left">
                <a class="level-item" aria-label="reply"
                  href="https://yunzeman.github.io/situation3d/static/Situation3D_CVPR2024.pdf">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  Paper
                </a>
                <a class="level-item" aria-label="retweet"
                  href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/30234.png">
                  <span class="icon">
                    <i class="fas fa-chalkboard-teacher"></i>
                  </span>
                  Poster
                </a>
              </div>
            </nav>
          </div>
        </article>
      </div>


      <div class="box">
        <article class="media">
          <div class="media-content">
            <div class="content">
              <p>
                <strong>Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene
                  Representation</strong>
                <br>
                Taisei Hanyu, Kashu Yamazaki, Benjamin R Runkle, Ngan Le
              </p>
            </div>
            <nav class="level is-mobile">
              <div class="level-left">
                <a class="level-item" aria-label="reply"
                  href="https://onedrive.live.com/?redeem=aHR0cHM6Ly8xZHJ2Lm1zL2YvYy9jOWYyZmVhNTEyMWIwOTgyL0V1Qk0yODBkQXRwQm4xN2NucEVybThVQlgyMnhSaFd3TXRpTVN4cHdPN01SSUE%5FZT1TVkFYSE4&cid=C9F2FEA5121B0982&id=C9F2FEA5121B0982%21s7411459a843e46f2992784659e4478c1&parId=C9F2FEA5121B0982%21scddb4ce0021d41da9f5edc9e912b9bc5&o=OneUp">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  Paper
                </a>
                <a class="level-item" aria-label="retweet"
                  href="https://onedrive.live.com/?redeem=aHR0cHM6Ly8xZHJ2Lm1zL2YvYy9jOWYyZmVhNTEyMWIwOTgyL0V1Qk0yODBkQXRwQm4xN2NucEVybThVQlgyMnhSaFd3TXRpTVN4cHdPN01SSUE%5FZT1TVkFYSE4&cid=C9F2FEA5121B0982&id=C9F2FEA5121B0982%21sce972af29c25485b82851364ef9f58ee&parId=C9F2FEA5121B0982%21scddb4ce0021d41da9f5edc9e912b9bc5&o=OneUp">
                  <span class="icon">
                    <i class="fas fa-chalkboard-teacher"></i>
                  </span>
                  Poster
                </a>
              </div>
            </nav>
          </div>
        </article>
      </div>

    </div>
  </section>

  <section class="section" id="Organizers">
    <div class="container is-max-desktop content">
      <h2 class="title" id="organizers">Organizers</h2>

      <div class="columns is-centered is-variable is-0">
        <div class="column is-one-quarter">
          <a href="http://francisengelmann.github.io">
            <div class="card">
              <div class="card-image">
                <figure class="image">
                  <img class="is-rounded" src="./static/img/francis.jpg" alt="Placeholder image">
                </figure>
              </div>
              <div class="card-content">
                <div class="media">
                  <div class="media-content" style="overflow-x: unset;">
                    <p class="title is-7 is-spaced">Francis Engelmann</p>
                    <p class="subtitle is-7">ETH Zurich</p>
                  </div>
                </div>
              </div>
            </div>
          </a>
        </div>
        <div class="column is-one-quarter">
          <a href="https://aycatakmaz.github.io">
            <div class="card">
              <div class="card-image">
                <figure class="image">
                  <img class="is-rounded" src="./static/img/ayca.jpg" alt="Placeholder image">
                </figure>
              </div>
              <div class="card-content">
                <div class="media">
                  <div class="media-content" style="overflow-x: unset;">
                    <p class="title is-7 is-spaced">Ayca Takmaz</p>
                    <p class="subtitle is-7">ETH Zurich</p>
                  </div>
                </div>
              </div>
            </div>
          </a>
        </div>
        <div class="column is-one-quarter">
          <a href="https://jonasschult.github.io">
            <div class="card">
              <div class="card-image">
                <figure class="image">
                  <img class="is-rounded" src="https://jonasschult.github.io/images/profile.jpg"
                    alt="Placeholder image">
                </figure>
              </div>
              <div class="card-content">
                <div class="media">
                  <div class="media-content" style="overflow-x: unset;">
                    <p class="title is-7 is-spaced">Jonas Schult</p>
                    <p class="subtitle is-7">RWTH Aachen</p>
                  </div>
                </div>
              </div>
            </div>
          </a>
        </div>
        <div class="column is-one-quarter">
          <a href="https://github.com/elisabettafedele">
            <div class="card">
              <div class="card-image">
                <figure class="image">
                  <img class="is-rounded" src="./static/img/elisabetta.png" alt="Placeholder image">
                </figure>
              </div>
              <div class="card-content">
                <div class="media">
                  <div class="media-content" style="overflow-x: unset;">
                    <p class="title is-7 is-spaced">Elisabetta Fedele</p>
                    <p class="subtitle is-7">ETH Zurich</p>
                  </div>
                </div>
              </div>
            </div>
          </a>
        </div>

      </div>
      <div class="columns is-centered is-variable is-0">

        <div class="column is-one-quarter">
          <a href="https://alexdelitzas.github.io/">
            <div class="card">
              <div class="card-image">
                <figure class="image">
                  <img class="is-rounded" src="./static/img/alex.jpeg" alt="Placeholder image">
                </figure>
              </div>
              <div class="card-content">
                <div class="media">
                  <div class="media-content" style="overflow-x: unset;">
                    <p class="title is-7 is-spaced">Alex Delitzas</p>
                    <p class="subtitle is-7">ETH Zurich</p>
                  </div>
                </div>
              </div>
            </div>
          </a>
        </div>


        <div class="column is-one-quarter">
          <a href="https://github.com/WaldJohannaU">
            <div class="card">
              <div class="card-image">
                <figure class="image">
                  <img class="is-rounded" src="./static/img/johanna.jpg" alt="Placeholder image">
                </figure>
              </div>
              <div class="card-content">
                <div class="media">
                  <div class="media-content" style="overflow-x: unset;">
                    <p class="title is-7 is-spaced">Johanna Wald</p>
                    <p class="subtitle is-7">Google</p>
                  </div>
                </div>
              </div>
            </div>
          </a>
        </div>
        <div class="column is-one-quarter">
          <a href="https://pengsongyou.github.io">
            <div class="card">
              <div class="card-image">
                <figure class="image">
                  <img class="is-rounded" src="./static/img/songyou.jpg" alt="Placeholder image">
                </figure>
              </div>
              <div class="card-content">
                <div class="media">
                  <div class="media-content" style="overflow-x: unset;">
                    <p class="title is-7 is-spaced">Songyou Peng</p>
                    <p class="subtitle is-7">ETH Zurich</p>
                  </div>
                </div>
              </div>
            </div>
          </a>
        </div>
        <div class="column is-one-quarter">
          <a href="https://xiwang1212.github.io/homepage/">
            <div class="card">
              <div class="card-image">
                <figure class="image">
                  <img class="is-rounded" src="./static/img/xi.jpg" alt="Placeholder image">
                </figure>
              </div>
              <div class="card-content">
                <div class="media">
                  <div class="media-content" style="overflow-x: unset;">
                    <p class="title is-7 is-spaced">Xi Wang</p>
                    <p class="subtitle is-7">ETH Zurich</p>
                  </div>
                </div>
              </div>
            </div>
          </a>
        </div>

      </div>
      <div class="columns is-centered is-variable is-0">


        <div class="column is-one-quarter">
          <a href="https://orlitany.github.io">
            <div class="card">
              <div class="card-image">
                <figure class="image">
                  <img class="is-rounded" src="./static/img/or.jpg" alt="Placeholder image">
                </figure>
              </div>
              <div class="card-content">
                <div class="media">
                  <div class="media-content" style="overflow-x: unset;">
                    <p class="title is-7 is-spaced">Or Litany</p>
                    <p class="subtitle is-7">NVIDIA, Technion</p>
                  </div>
                </div>
              </div>
            </div>
          </a>
        </div>
        <div class="column is-one-quarter">
          <a href="https://paschalidoud.github.io">
            <div class="card">
              <div class="card-image">
                <figure class="image">
                  <img class="is-rounded" src="./static/img/despoina.jpeg" alt="Placeholder image">
                </figure>
              </div>
              <div class="card-content">
                <div class="media">
                  <div class="media-content" style="overflow-x: unset;">
                    <p class="title is-7 is-spaced">Despi Paschalidou</p>
                    <p class="subtitle is-7">Stanford University</p>
                  </div>
                </div>
              </div>
            </div>
          </a>
        </div>


        <div class="column is-one-quarter">
          <a href="https://federicotombari.github.io">
            <div class="card">
              <div class="card-image">
                <figure class="image">
                  <img class="is-rounded" src="./static/img/federico.jpg" alt="Placeholder image">
                </figure>
              </div>
              <div class="card-content">
                <div class="media">
                  <div class="media-content" style="overflow-x: unset;">
                    <p class="title is-7 is-spaced">Federico Tombari</p>
                    <p class="subtitle is-7">TUM, Google</p>
                  </div>
                </div>
              </div>
            </div>
          </a>
        </div>
        <div class="column is-one-quarter">
          <a href="https://people.inf.ethz.ch/marc.pollefeys/">
            <div class="card">
              <div class="card-image">
                <figure class="image">
                  <img class="is-rounded" src="./static/img/marc.jpg" alt="Placeholder image">
                </figure>
              </div>
              <div class="card-content">
                <div class="media">
                  <div class="media-content" style="overflow-x: unset;">
                    <p class="title is-7 is-spaced">Marc Pollefeys</p>
                    <p class="subtitle is-7">ETHZ, Microsoft</p>
                  </div>
                </div>
              </div>
            </div>
          </a>
        </div>

        <!-- <div class="column is-one-fifth">
            <a href="https://geometry.stanford.edu/member/guibas/">
              <div class="card">
                <div class="card-image">
                  <figure class="image">
                    <img class="is-rounded" src="./static/img/leonidas.jpg" alt="Placeholder image">
                  </figure>
                </div>
                <div class="card-content">
                  <div class="media">
                    <div class="media-content" style="overflow-x: unset;">
                      <p class="title is-7 is-spaced">Leonidas Guibas</p>
                      <p class="subtitle is-7">Stanford University</p>
                    </div>
                  </div>
                </div>
              </div>
            </a>
          </div> -->

      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content">
        <p>
          This website is licensed under a <a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          <br />
          It borrows the source code of <a href="https://github.com/nerfies/nerfies.github.io">this website</a>.
          We would like to thank Utkarsh Sinha and Keunhong Park.
        </p>
      </div>
    </div>
  </footer>

</body>
<script src="js/jquery-2.1.1.js"></script>
<script src="js/jquery.mobile.custom.min.js"></script> <!-- Resource jQuery -->
<script src="js/main.js"></script> <!-- Resource jQuery -->

</html>
